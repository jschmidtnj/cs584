\documentclass{article}

\usepackage{arxiv}
\setcounter{secnumdepth}{2} 
\fancyfoot{}
\sloppy
\usepackage{wrapfig}
\usepackage{booktabs} % For formal tables
\usepackage{caption}
\usepackage{subcaption}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithmic}
\usepackage[numbers]{natbib}
\usepackage{comment}
\usepackage[linesnumbered,ruled,vlined]{algorithm2e}
\SetKwComment{Comment}{$\triangleright$\ }{}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{multirow}
\usepackage{hyperref}
\usepackage[table, svgnames, dvipsnames]{xcolor}
\usepackage{makecell, cellspace, caption}

\newcommand{\red}[1]{{\color{red}#1}}

\title{CS584: Multilingual Toxic Comment Classification} % \\

\author{
Joshua Schmidt\textsuperscript{1}\\
\textsuperscript{1}{Stevens Institute of Technology}\\
jschmid3@stevens.edu
}
\date{\today}
\begin{document}
\maketitle
\begin{abstract}

Most comments on online social platforms are constructive, increasing human interactions and quality of life. However, with the immense amount of information produced daily there is a rise in online personal attacks, harassment, and cyber-bullying with toxic comments. This has triggered the research community to identify an efficient and accurate model for predicting the toxicity of online comments, but so far there is no clear solution. Due to the global nature of this problem, any classification model would need to work with many foreign languages and on all online platforms. In this report, several machine learning model designs are explored as to their effectiveness in this classification problem. The model designs were evaluated based on their accuracy and speed, utilizing datasets from the "Multilingual Toxic Comment Classification" Kaggle competition. XML Roberta had the best performance, but other models with simpler architectures also performed surprisingly well.

\end{abstract}

\section{Introduction}

In this hyper-partisan and polarizing time, with disinformation, increased online activity, and hate speech on the rise, it is imperative to promote healthy and progressive conversations. It is very easy for internet "trolls" to hide behind anonymity and spread toxic comments, degrading online conversations and promoting dispute. In this context, \textit{toxicity} is defined as \textit{anything rude, disrespectful, or otherwise reducing discussion engagement}. It is impossible to completely moderate large social media platforms such as Instagram or Twitter, as the amount of information uploaded daily is enormous. If these platforms decided to check all of their content by hand, it would require such a large workforce that their businesses would no longer be profitable. Completely automated moderation does not work either, as there are nuances to each conversation that no modern system would be able to understand, especially considering the scale and diversity of the userbases.

The best solution to online content moderation is a system that has automated and human components working in tandem. Automated systems can flag comments as potentially toxic within a confidence interval, and either immediately block the comment, or request a human moderator to review it. This is a delicate balance, as platforms need to promote free speech, encourage good conversations, and minimize false-positives. Platforms that fail on any of these points, due to too little, too much, or otherwise insufficient moderation, can lose users or incur government reprisal.

In this project, the goal was to create an automated machine learning system for classifying online comments as toxic or insightful, with confidence scores. The system needed to support multiple languages, as this problem affects social media platforms globally. The main metric used for evaluation was the accuracy of the classifications.

\section{Background / Related work}

This project used a Kaggle Competition \cite{kaggle_competition} for the datasets and problem statement. As such, many individuals have already attempted to solve the "Toxic Comment Classification" challenge. However, the code teams used for attempting the challenge is private, so their exact approaches are unknown.

In literature, there is considerable interest in the detection of toxic comments \cite{almerekhi2019detecting} due to its varied nature from a linguistic perspective. In \cite{vanaken2018challenges}, researchers described the many challenges in detecting toxic comments. They proposed an ensemble method, which combines several different classifiers, and is proven to work well with different vocabularies. Their ensemble method produced results with accuracies of approximately 0.68. Another team of researchers in \cite{8660863} created a detector for abusive comments in Bengala Facebook pages, and claimed that an SVM with a linear kernel performs better when a TF-IDF vectorizer is used. This SVM and TF-IDF combination resulted in classification accuracy scores of approximately 0.58. In relation, researchers in \cite{obadimu2019identifying} experimented with detecting the toxicity of YouTube comments, applying Latent Dirichlet Allocation to determine the topics of the toxic comments. The most successful of these papers used deep learning-based models \cite{deep_learning_approaches} (CNN's, RNN's, etc), and their results seemed to translate well from one platform to another (YouTube to Facebook, for example). These models were not perfect, and had accuracy scores of at most 0.38 for a holistic dataset \cite{deep_learning_approaches}.

Unfortunately, there seems to be little research on how accurately toxicity can be detected with multiple languages. It is not known how introducing a dataset with comments in different languages will affect overall model performance, but intuitively more comments will be required to create a holistic model. There also is little research on the application of transformers to multi-lingual toxicity classification, using one generalized model (or set of models). It is notable that current state of the art systems such as Google's \textbf{Perspective} api can be deceived through spelling mistakes \cite{hosseini2017deceiving}. This will need to be addressed in this project as well.

\section{Approach}

There are several approaches that were tested in this paper. One goal of this project was to create each layer of the transformers architecture separately, with the layers being a simple RNN, LSTM, bi-directional LSTM, and GRU \cite{lample2019crosslingual}. That way the impact of each of the layers of the architecture could be compared to the performance of the transformers architecture as a whole, and it would be straight-forward to determine if the final layers are even necessary to achieve good performance. More elementary NLP models, such as RandomForest and n-grams, were not explored in this project because the literature review revealed that these models do not have good performance with multiple languages and smaller datasets.

The final models tested were those based on the transformers architecture, pre-trained on multiple languages. These models are known as cross-lingual language models (XLM)s \cite{lample2019crosslingual}. There were two models tested - Multilingual DistilBERT, and XLM Roberta. XLM Roberta is currently one of the best transformers for cross-language classification \cite{deep_learning_approaches}, and Multilingual DistilBERT also performs well.

After creating the XLM transformers models, the final step of the project was to determine the best input parameters and architecture. After this optimization, the model performances were evaluated and compared to the different layers in the transformers architecture. Through this approach, it is possible to distinguish which layers of the transformers architecture provide the greatest impact on the accuracy of the output, and decrease the model complexity if any layers result in negligible changes in the output.

\subsection{Evaluation}

This system was scored based on the speed and accuracy of classification. Model size and training time were not considered; any problem arising from them can be mitigated by allocating additional resources and optimizing implementations. The final score was weighted 80\% accuracy and 20\% speed because increased error may exacerbate the current problems with toxic comments in social media \cite{10.1145/3200947.3208069}. The more accurate the model is, the less human intervention is required, and the less false-negatives slip through the classification system. Prediction speed is important to ensure timely communication, but it is not critical.

\section{Experimental Design}

\subsection{Software}

The software used for this experiment was written mainly using the Tensorflow 2 framework. Scikit-Learn, Pandas, Numpy, and MatplotLib were used for data manipulation and visualization, while Tensorflow and Keras were used for constructing and running all of the models. Tensorflow was selected because it is widely adopted in industry, and there are many cloud services that accelerate training with Tensorflow models. For building the transformers models, the Hugging Face Transformers library was used because of its compatibility with Tensorflow and its support for both DistilBERT and XLM Roberta.

\subsection{Data Sets}

The datasets used for this experiment were provided by the Kaggle competition. Of these, two are from previous competitions --- \textit{toxic comments} and \textit{comments with unintended bias}. These datasets contain 223,549 and 1,876,468 rows, respectively, with each row consisting of a unique id, the comment text, and its multi-label classification --- toxic, severe\_toxicity, obscene, insult, threat. These datasets were combined and used as training data.

The \textit{toxic comments} dataset contains an additional label - identity\_hate, which is not present in the second training dataset. Therefore this label is be ignored. Out of the 223,549 rows, there are only 21,384 toxic comments, which is about 10\% of the dataset. There are 1,962 extremely toxic comments (0.88\%), 12,140 obscene comments (5.4\%), 689 threats (0.3\%), and 11,304 insults (5\%). All of the comments are written in English.

The \textit{comments with unintended bias} dataset contains many more labels that do not exist in the \textit{toxic comments} dataset (asian, atheist, female, muslim, etc.). Since this project is focused on classifying toxicity, and not on identity bias, these extra fields were ignored. The toxicity in this dataset is not binary, and is instead in a range of values from 0 to 1. The mean toxicity is 0.1 with a standard deviation of 0.2. This range of values was converted to binary in order to match the first dataset. Severe toxicity has a mean of 0.001 with a standard deviation of 0.02; obscene 0.01 and 0.06, insult 0.08 and 0.18, and threat 0.01 and 0.05, respectively. All of the comments are written in English.

In total in the training data, there were 2,133,743 comments provided, of which 1,998,903 are normal and 134,840 are toxic. This is a large dataset, which made it ideal for training the transformers models.

After training, there were two additional datasets provided, one called "validation" and another called "test". The validation dataset contains 8,000 rows of comments and binary toxic classifications. There are 1,230 toxic comments (15.4\%). The comments in the validation dataset are written in several foreign languages. The test dataset was used for submission to the competition, containing 15\% of the complete competition test dataset. The test dataset contains 63,812 rows of comments and languages the comments were written in.

\subsection{Model Architecture}

When creating the different machine learning models, the goal was to keep the architecture as simple as possible, so that it would be possible to change only several hyperparameters at a time whn optimizing. The Simple RNN, LSTM, GRU, and bi-directional LSTM all used binary cross-entropy for the loss function, and the adam optimizer for optimization. These were chosen because they are the same as those used in the transformers models. 

The Simple RNN contained three layers - an embedding layer with an output shape 1500x300, a SimpleRNN layer of size 100, and a dense layer to output the binary classification. A Keras tokenizer was used to normalize the input text by removing punctuation and making it lowercase, and convert the text into integer vectors with TF-IDF. Different internal units and output shapes were tested for both the SimpleRNN and embedding layer, respectively, in multiples of 50. But these were the parameters that resulted in the best accuracy.

The LSTM, GRU, and bi-directional LSTM all used the same pretrained GloVe word embedding layer. The GloVe embeddings data was found in a separate kaggle dataset, from "glove840b300dtxt".

The simple LSTM contained three layers - the aforementioned embedding layer (with an output shape of 1500x300), an LSTM layer of size 100, and a dense layer to output the binary classification. Different internal units and outputs were tested for both the LSTM and embedding layers, in multiples of 25, but these parameters resulted in the highest accuracy.

The GRU contained four layers - the same embedding layer, a one-dimensional spatial dropout with a probability of 0.3, a GRU layer of size 300, and a binary dense layer. Different spacial dropouts and GRU internal sizes were tested, but again these parameters resulted in the highest accuracy.

The bi-directional LSTM contained three layers - the same embedding layer, a bidirectional LSTM layer with dropout and recurrent dropout of 0.3, and a binary dense layer. In this model, the only hyperparameters that were tested were the dropouts and internal unit size. The internal unit size was modified by multiples of 25, and the dropout by 0.1, with the parameters chosen based off the combination with the highest accuracy.

The transformer models were a bit different in that they each had individual tokenizers. Both of the tokenizers for these models split the text into vectors of words, add tokens for the start and end of the sentence, and add padding to make each vector the same length. Then they convert the text to numeric values so that it is ready for training input.

Both the DistilBERT and XLM Roberta models contained four layers - an input layer, the transformer layer (either DistilBERT or XLM Roberta), and two dimensionality reduction layers. The first dimensional reduction layer is specific to the given transformer, and the second is a dense layer, converting the output to a binary classification. The one hyperparameter that was optimized was the shape of the input words. 192 words is the longest message length in the dataset, and making the input size larger had a slightly negative impact on the accuracy of the model.

\section{Experimental Results}

To compare all of the models, the primary metrics used were the area under the ROC curve and the training and validation loss curves. By looking at the training and validation loss curves, it is easy to see if the model is overfit, underfit, if the data is unrepresentative, or a good fit. These loss curves can be found in Figure \ref{fig:train_validation}. Models with higher areas under the ROC curve are better at classifying the toxic comments. Precision and recall were ignored because they would not be as useful as area under the ROC curve in determining the most accurate classification model. All of the models have relatively fast prediction times (at least 10 predictions per second), so the main metric used for comparison is area under the ROC curve. The training was conducted on an AWS "ml.p2.xlarge" instance.

\begin{figure}
\begin{tabular*}{\textwidth}{c @{\extracolsep{\fill}} cc}
\subfloat[Simple RNN]{\includegraphics[width = 3in]{../code/output/simple_rnn.jpg}} &
\subfloat[LSTM]{\includegraphics[width = 3in]{../code/output/lstm.jpg}} \\
\subfloat[GRU]{\includegraphics[width = 3in]{../code/output/gru.jpg}} &
\subfloat[bidirectional LSTM]{\includegraphics[width = 3in]{../code/output/bidirectional_lstm.jpg}} \\
\subfloat[DistilBERT]{\includegraphics[width = 3in]{../code/output/distilbert.jpg}} &
\subfloat[XLM Roberta]{\includegraphics[width = 3in]{../code/output/xlm_roberta.jpg}}
\end{tabular*}
\caption{Training and Validation Loss}
\label{fig:train_validation}
\end{figure}

The Simple RNN, LSTM, GRU and bidirectional LSTM all trained for 15 epochs. With the Simple RNN model, the resulting accuracy was 90.86\%. It took 5 minutes to train, and the area under the ROC curve was 68.726\%. This model was relatively fast to train, but the performance is far worse compared to the other models studied, as expected.

The LSTM model training resulted in an accuracy score of 94.72\%. It took 12 minutes to train, and the area under the ROC curve was 96.241\%. It was expected for the LSTM model to do better than the Simple RNN, but it was surprising that the area under the ROC curve was so high already.

After training the GRU model, the accuracy score was 95.51\%. The training process took approximately 18 minutes on the AWS instance. The area under the ROC curve was 97.132\%.

The final base non-transformer model, the bidirectional LSTM, had a slightly lower accuracy score of 94.59\%. The training process took approximately 20 minutes, and the area under the ROC curve was 97.380\%.

The two transformers models were trained over 30 epochs. The first model, DistilBERT, had a training accuracy of 96.97\%. Training took approximately 25 minutes, with the area under the ROC curve being 98.240\%. The second model, XLM Roberta, had a training accuracy of 97.99\%. The training process took approximately 26 minutes, with the resulting area under the ROC curve being 98.723\%.

After training and gathering these results, the best two models were scored with the test data from the competition. These two best models were the two transformers models unsurprisingly. The DistilBERT model scored 87.03\%, and the XLM Roberta model scored 93.82\%. It is surprising that the scores for these two models were so high because they were only trained on English messages, and tested on messages from many different languages. The pre-training for cross-language support is what made the models work so well.

It was also surprising that the simple LSTM model was able to classify toxic comments so accurately. Because there is no cross-language training, it would not do well in the test set for this competition. But if LSTM models are trained on different languages, and there would be one model per language, that may be a more cost-effective way of creating an accurate classifier. One main issue with that approach is one would need a separate algorithm to detect what language the message is written in, in order to forward the message to the corresponding model.

\section{Conclusion and Future Work}

In conclusion, this experiment turned out very successful. Many of the original hypothesis were validated, such as XLM Roberta would have the best performance and the Simple RNN would have the worst. However there were also many surprises, like how accurate the LSTM model is on English messages. The experiment was limited in several different ways. The dataset was large, but only came from a single source (Twitter) and did not have good labeled metadata of population demographics. Therefore it is possible that the models we have created exhibit racial or demographic bias. The data was collected approximately one year ago, and the way that people communicate may have changed slightly in that time, affecting the performance of these models. There also was limited training time due to budget constraints, preventing training for more epochs or testing more model parameters.

Future work should address these challenges and answer some of the questions they pose. How does hate speech differ on different platforms, if at all? Do the generated models have inherent bias, and if so, what type of training data is required to circumvent this? How can we make these transformer models more efficient to train, so that it does not require as many resources to create the state-of-the-art classification systems? How do other cross-language models compare to the performance of XLM Roberta and DistilBERT? There are many different paths for future exploration, and many questions left to answer. But from this work, it is clear that cross-language transformers models are effective at classifying hate speech on social media platforms. With some slight oversight from human content moderators, this could be an effective tool towards promoting safer and more productive conversations online.

\begin{footnotesize}
\bibliographystyle{plainnat}
\bibliography{ref}
\end{footnotesize}
\end{document}
