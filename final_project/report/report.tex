\documentclass{article}

\usepackage{arxiv}
\setcounter{secnumdepth}{2} 
\fancyfoot{}
\sloppy
\usepackage{wrapfig}
\usepackage{booktabs} % For formal tables
\usepackage{caption}
\usepackage{subcaption}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithmic}
\usepackage[numbers]{natbib}
\usepackage{comment}
\usepackage[linesnumbered,ruled,vlined]{algorithm2e}
\SetKwComment{Comment}{$\triangleright$\ }{}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{multirow}
\usepackage{hyperref}
\usepackage[table, svgnames, dvipsnames]{xcolor}
\usepackage{makecell, cellspace, caption}

\newcommand{\red}[1]{{\color{red}#1}}

\title{CS584: Multilingual Toxic Comment Classification} % \\

\author{
Joshua Schmidt\textsuperscript{1}\\
\textsuperscript{1}{Stevens Institute of Technology}\\
jschmid3@stevens.edu
}
\date{\today}
\begin{document}
\maketitle
\begin{abstract}

Most comments on online social platforms are constructive, increasing human interactions and quality of life. However, with the immense amount of information produced daily there is a rise in online personal attacks, harassment, and cyber-bullying with toxic comments. This has triggered the research community to identify an efficient and accurate model for predicting the toxicity of online comments, but so far there is no clear solution. Due to the global nature of this problem, any classification model would need to work with many foreign languages and on all online platforms. In this work, a project is proposed for exploring new model designs based on transformers and specifically XML Roberta. The model designs will be evaluated based on accuracy and speed, utilizing datasets from the "Multilingual Toxic Comment Classification" Kaggle competition.

\end{abstract}

\section{Introduction}

In this hyper-partisan and polarizing time, with disinformation, increased online activity, and hate speech on the rise, it is imperative to promote healthy and progressive conversations. It is very easy for internet "trolls" to hide behind anonymity and spread toxic comments, degrading online conversations and promoting dispute. In this context, \textit{toxicity} is defined as \textit{anything rude, disrespectful, or otherwise reducing discussion engagement}. It is impossible to completely moderate large social media platforms such as Instagram or Twitter, as the amount of information uploaded daily is enormous. If these platforms decided to check all of their content by hand, it would require such a large workforce that their businesses would no longer be profitable. Completely automated moderation does not work either, as there are nuances to each conversation that no modern system would be able to understand, especially considering the scale and diversity of the userbase.

The best solution to online content moderation is a system that has automated and human components working in tandem. Automated systems can flag comments as potentially toxic within a confidence interval, and either immediately block the comment, or request a human moderator to review it. This is a delicate balance, as platforms need to promote free speech, encourage good conversations, and minimize false-positives. Platforms that fail on any of these points, due to too little, too much, or otherwise insufficient moderation, can lose users or incur government reprisal.

In this project, the goal is to create an automated machine learning system for classifying online comments as toxic or insightful, with confidence scores. The system needs to support multiple languages, as this problem affects social media platforms globally. The main metric used for evaluation will be the accuracy of the classifications.

\section{Background / Related work}

This project uses a Kaggle Competition \cite{kaggle_competition} for the datasets and problem statement. As such, many teams have already attempted to solve the "Toxic Comment Classification" challenge. However, the code teams used for attempting the challenge is private, so their exact approaches are unknown.

In literature, there is considerable interest in the detection of toxic comments \cite{almerekhi2019detecting} due to its varied nature from a linguistic perspective. In \cite{vanaken2018challenges}, researchers described the many challenges in detecting toxic comments. They proposed an ensemble method, which combines several different classifiers, and is proven to work well with varied vocabularies. Another team of researchers in \cite{8660863} created a detector for abusive comments in Bengala Facebook pages, and claimed that an SVM with a linear kernel performs better when a TF-IDF vectorizer is used. In relation, researchers in \cite{obadimu2019identifying} experimented with detecting the toxicity of YouTube comments, applying Latent Dirichlet Allocation to determine the topics of the toxic comments. The most successful of these papers used deep learning-based models \cite{deep_learning_approaches} (CNN's, RNN's, etc), and their results seemed to translate well from one platform to another (YouTube to Facebook, for example). These models are not perfect, and have accuracy scores of at most 0.38 for a holistic dataset \cite{deep_learning_approaches}.

Unfortunately, there seems to be little research on how accurately toxicity can be detected with multiple languages. It is not known how introducing a dataset with comments in different languages will affect overall model performance, but intuitively more comments will be required to create a holistic model. There also is little research on the application of transformers to multi-lingual toxicity classification, using one generalized model (or set of models). It is notable that current state of the art systems such as Google's \textbf{Perspective} api can be deceived through spelling mistakes \cite{hosseini2017deceiving}. This will need to be addressed in this project as well.

\section{Approach}

\subsection{Data Sets}

There are several provided datasets for the Kaggle competition. Of these, two are from previous competitions --- \textit{toxic comments} and \textit{comments with unintended bias}. These datasets contain 223,549 and 187,6468 rows, respectively, with each row consisting of a unique id, the comment text, and its multi-label classification --- toxic, severe\_toxicity, obscene, insult, threat. These datasets will be used for this project, as training data.

The \textit{toxic comments} dataset contains an additional label - identity\_hate, which is not present in the second training dataset. Therefore this label will be ignored. Out of the 223,549 rows, there are only 21,384 toxic comments, which is about 10\% of the dataset. There are 1,962 extremely toxic comments (0.88\%), 12,140 obscene comments (5.4\%), 689 threats (0.3\%), and 11,304 insults (5\%). All of the comments are written in English.

The \textit{comments with unintended bias} dataset contains many more labels that do not exist in the \textit{toxic comments} dataset (asian, atheist, female, muslim, etc.). But since this project is focused on classifying toxicity, and not on identity bias, these extra fields will be ignored. The toxicity in this dataset is not binary, and is instead in a range of values from 0 to 1. The mean toxicity is 0.1 with a standard deviation of 0.2. Since this is a range of values, it may provide better results than the first dataset, which will have values skewed to extremes because of its binary classification. Severe toxicity has a mean of 0.001 with a standard deviation of 0.02; obscene 0.01 and 0.06, insult 0.08 and 0.18, and threat 0.01 and 0.05, respectively. All of the comments are written in English.

After training, there are two additional datasets provided, one called "validation" and another called "test". The validation dataset contains 8,000 rows of comments and binary toxic classifications. There are 1,230 toxic comments (15.4\%). The comments in the validation dataset are written in several foreign languages. The test dataset is used for submission to the competition, containing 15\% of the complete competition test dataset. The test dataset contains 63,812 rows of comments and languages the comments were written in.

\subsection{Method}

There are several approaches that will be tested in this paper. The final product will use a cross-lingual language model (XLM) to evaluate and classify sentences \cite{lample2019crosslingual}. This model will use the pre-trained XML Roberta transformer, which is currently one of the best transformers for cross-language classification \cite{deep_learning_approaches}. This transformers architecture is based on a simple RNN, LSTM, bi-directional LSTM, and GRU \cite{lample2019crosslingual}. Another goal of this project is to create each layer of the underlying architecture separately, and evaluate how the accuracy of the model increases with the addition of each layer.

Once the XML transformers model is created, the next goal of the project is to determine the best input parameters and architecture. Once this is optimized, the model performance will be evaluated and compared to the BERT transformers architecture. BERT does not have cross-lingual pre-training, and it will be interesting to witness what effect its absence has on the overall performance.

\subsection{Evaluation}

This system will be scored based on the speed and accuracy of classification. Model size and training time will not be considered; any problem arising from them can be mitigated by allocating additional resources and optimizing implementations. The final score will be weighted 80\% accuracy and 20\% speed because increased error may exacerbate the current problems with toxic comments in social media \cite{10.1145/3200947.3208069}. The more accurate the model is, the less human intervention is required, and the less false-negatives slip through the classification system. Prediction speed is important to ensure timely communication, but it is not critical.

\section{Experimental Design}

\section{Experimental Results}

\section{Analysis}

\section{Conclusion and Future Work}

\begin{footnotesize}
\bibliographystyle{plainnat}
\bibliography{ref}
\end{footnotesize}%
\end{document}
