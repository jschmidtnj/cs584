\documentclass{article}

\usepackage{arxiv}
\setcounter{secnumdepth}{2} 
\fancyfoot{}
\sloppy
\usepackage{wrapfig}
\usepackage{booktabs} % For formal tables
\usepackage{caption}
\usepackage{subcaption}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithmic}
\usepackage[numbers]{natbib}
\usepackage{comment}
\usepackage[linesnumbered,ruled,vlined]{algorithm2e}
\SetKwComment{Comment}{$\triangleright$\ }{}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{multirow}
\usepackage{hyperref}
\usepackage[table, svgnames, dvipsnames]{xcolor}
\usepackage{makecell, cellspace, caption}

\newcommand{\red}[1]{{\color{red}#1}}

\title{CS584: Multilingual Toxic Comment Classification} % \\

\author{
Joshua Schmidt\textsuperscript{1}\\
\textsuperscript{1}{Stevens Institute of Technology}\\
jschmid3@stevens.edu
}
\date{\today}
\begin{document}
\maketitle
\begin{abstract}

Most comments on online social platforms are constructive, increasing human interactions and quality of life. However, with the immense amount of information produced daily there is a rise in online personal attacks, harassment, and cyber-bullying with toxic comments. This has triggered the research community to identify an efficient and accurate model for predicting the toxicity of online comments, but so far there is no clear solution. Due to the global nature of this problem, any classification model would need to work with many foreign languages and on all online platforms. In this work, a project is proposed for exploring new model designs based on transformers and specifically XML Roberta. The model designs will be evaluated based on accuracy and speed, utilizing datasets from the "Multilingual Toxic Comment Classification" Kaggle competition.

\end{abstract}

\section{Introduction}

In this hyper-partisan, polarizing time, with disinformation, increased online activity, and growing hate speech, it is imperative to promote healthy and progressive conversations. However, it is very easy for internet "trolls" to hide behind anonymity and spread toxic comments, degrading online conversations and promoting dispute. In this context, \textit{toxicity} is defined as \textit{anything rude, disrespectful, or otherwise lowering discussion engagement}. It is impossible to completely moderate large social media platforms such as Instagram or Twitter, as the amount of information uploaded every day is enormous. If these platforms decided to check all of their content by hand, it would require such a large workforce that their businesses would no longer be profitable. Completely automated moderation does not work either, as there are nuances to each conversation that no sophisticated system would be able to understand, especially with a large and dynamic userbase.

The best solution to online content moderation is a system that has automated and human components working in tandem. Automated systems can flag comments as potentially toxic within a confidence interval, and either immediately block the comment or request a human moderator to review it. This is a delicate balance, as platforms need to promote free speech, encourage good conversations, and minimize false-positives. Platforms that fail on any of these points, due to too little, too much, or otherwise insufficient moderation, can lose users or incur government retaliation.

In this project, the goal is to create an automated machine learning system for classifying online comments as toxic or insightful, with confidence scores. The system needs to support multiple languages, as this problem affects social media platforms in all countries they are available. The main metric used for evaluation will be the accuracy of the classifications.

\section{Background / Related work}

This project uses a Kaggle Competition \cite{kaggle_competition} for the datasets and problem statement. As such, many teams have already attempted to solve the "Toxic Comment Classification" challenge. However, the code teams used for attempting the challenge is private, so their exact approaches are unknown.

In literature, there is considerable interest in the detection of toxic comments \cite{almerekhi2019detecting} due to its varied nature from a linguistic perspective. In \cite{vanaken2018challenges}, researchers described the many challenges in detecting toxic comments. They proposed an ensemble method, which combines several different classifiers, works well with varied vocabularies. The researchers in \cite{8660863} created a detector for abusive comments in Bengala Facebook pages, and claimed that an SVM with a linear kernel performs better when a TF-IDF vectorizer is used. In relation, researchers in \cite{obadimu2019identifying} experimented with detecting the toxicity of YouTube comments, applying Latent Dirichlet Allocation to determine the topics on which the toxic comments were posted. The most successful of these papers used deep learning-based models \cite{deep_learning_approaches}, and their results seemed to translate well from one platform to another (YouTube to Facebook, for example).

However, there seems to be little research on how accurately toxicity can be detected with multiple languages. It is not known how introducing a dataset with comments in different languages will affect the overall performance, but intuitively more comments will be required to create a holistic model. There also is little research in the application of transformers on multi-lingual toxicity classification. It is notable that current state of the art systems such as Google's \textbf{Perspective} api can be deceived through spelling mistakes \cite{hosseini2017deceiving}. This will need to be addressed in this project as well.

\section{Data Sets}

There are several provided datasets for the Kaggle competition. There are two datasets from previous competitions --- \textit{toxic comments} and \textit{comments with unintended bias}. These datasets contain 223,549 and 187,6468 rows, respectively, with each row consisting of a unique id, the comment text, and different classifications --- toxic, severe\_toxicity, obscene, insult, threat. These datasets will be used for this project, as training data.

The \textit{toxic comments} dataset contains an additional label - identity\_hate, which is not present in the second training dataset. Therefore this label will be ignored. Out of the 223,549 rows, there are only 21,384 toxic comments, or about 10\% of the dataset. There are 1,962 extremely toxic comments (0.88\%), 12,140 obscene comments (5.4\%), 689 threats (0.3\%), and 11,304 insults (5\%). All of the comments are written in English.

The \textit{unintended bias} dataset contains many more labels that do not exist in the \textit{toxic comments} dataset (asian, atheist, female, muslim, etc.). But since this project is focused on classifying toxicity, and not on identity bias, these extra fields will be ignored. The toxicity in this dataset is not binary, and is instead in a range of values from 0 to 1. The mean toxicity is 0.1 with a standard deviation of 0.2. Since this is a range of values, it may provide better results than the first dataset, which will have values skewed to extremes because of its binary classification. Severe toxicity has a mean of 0.001 with a standard deviation of 0.02; obscene 0.01 and 0.06, insult 0.08 and 0.18, and threat 0.01 and 0.05, respectively. All of the comments are written in English.

After training, there are two additional datasets provided, one called "validation" and another called "test". The validation dataset contains 8,000 rows of comments and binary toxic classifications. There are 1,230 toxic comments (15.4\%). The comments in the validation dataset are written in several foreign languages. The test dataset is used for submission to the competition, containing 15\% of the complete competition test dataset. The test dataset contains 63,812 rows of comments and languages the comments were written in.

\section{Method}

There are several approaches that will be tested in this paper. The final product will use a cross-lingual language model (XLM) to evaluate and classify sentences \cite{lample2019crosslingual}. This model will use the pre-trained XML Roberta transformer, which is currently one of the best transformers for cross-language classification. This transformers architecture is based on a simple RNN, LSTM, bi-directional LSTM, and GRU \cite{lample2019crosslingual}. So another goal of this project is to create each layer of the underlying architecture separately, and evaluate how the accuracy of the model increases with the addition of each layer.

Once the XML transformers model is created, the next goal of the project is to determine the best input parameters and architecture. Once this is optimized, the model performance will be evaluated and compared to the BERT transformers architecture. BERT does not have cross-lingual pre-training, and it will be interesting to witness what effect its absence has on the overall performance.

\section{Evaluation}

This system will be scored on the speed of classification and accuracy. Model size and training time will not be used for model performance evaluation because social media giants have the resources and time to create much larger and more sophisticated models. The accuracy will have a higher weight, at 80\% vs. 20\% for speed, because it is much more important to minimize false classifications. Speed can be improved through increased resources and aftermath optimizations, but increased error may exacerbate the current problems with toxic comments on social media \cite{10.1145/3200947.3208069}. The more accurate the model is, the less human intervention is required, and the less false-negatives slip through the classification system. Prediction speed is important to ensure timely communication, but not as important as accuracy.

\begin{footnotesize}
\bibliographystyle{plainnat}
\bibliography{ref}
\end{footnotesize}%
\end{document}
